# Default values for diracx.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  # -- How long should batch jobs be retained after completing?
  batchJobTTL: 600
  # TODO: To avoid being unable to launch a container when the remote registry
  # is down this should be changed to IfNotPresent once we start using tags.
  # For now we override it to Always to avoid confusion around having an
  # outdated reference to the "latest" tag.
  imagePullPolicy: Always
  # What storage class should we use for DiracX volumes
  storageClassName: standard
  # -- timeout for job deadlines
  activeDeadlineSeconds: 900
  images:
    tag: "dev"
    services: ghcr.io/diracgrid/diracx/services
    client: ghcr.io/diracgrid/diracx/client
    busybox:
      tag: "latest"
      repository: "busybox"
    web:
      tag: "dev"
      repository: ghcr.io/diracgrid/diracx-web/static

# Number of diracx pods
replicaCount: 1
# Number of web pods
replicaCountWeb: 1

# Created with
# kubectl create secret generic regcred \
# --from-file=.dockerconfigjson=/home/chaen/.docker/config.json \
# --type=kubernetes.io/dockerconfigjson

# imagePullSecrets:
#   - name: regcred

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # -- Specifies whether a service account should be created
  create: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # fsGroup: 2000
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

#################

initSecrets:
  enabled: true
  rbac:
    create: true
  serviceAccount:
    enabled: true
    create: true
    name: # Specify a pre-existing ServiceAccount name

initSql:
  # Should DiracX include an init container which manages the SQL DB schema?
  enabled: true
  env: {}

initOs:
  enabled: true

initKeyStore:
  enabled: true

developer:
  enabled: true
  # -- Make it possible to launch the demo without having an internet connection
  offline: false
  # -- URLs which can be used to access various components of the demo (diracx, minio, dex, etc).
  # They are used by the diracx tests
  urls: {}
  # -- Path from which to mount source of DIRACX
  sourcePath: /diracx_source
  # -- List of packages which are mounted into developer.sourcePath and should be installed with pip install SOURCEPATH/...
  mountedPythonModulesToInstall: []
  # -- Use pip install -e for mountedPythonModulesToInstall
  # This is used by the integration tests because editable install might behave differently
  editableMountedPythonModules: true
  # -- Node module to install
  mountedNodeModuleToInstall: null
  # -- List of node workspace directories to manage in the diracx-web container (node_modules)
  nodeWorkspacesDirectories: []
  # -- Image to use for the webapp if nodeModuleToInstall is set
  nodeImage: node:alpine
  # -- Enable collection of coverage reports (intended for CI usage only)
  enableCoverage: false
  # -- Enable automatic reloading inside uvicorn when the sources change
  # Used by the integration tests for running closer to prod setup
  autoReload: true
  # -- If set, mount the CS stored localy instead of initializing a default one
  localCSPath: /local_cs_store
  # -- The IP that the demo is running at
  ipAlias: null

diracx:
  # -- Required: The hostname where the webapp/API is running
  hostname: ""
  # -- Settings to inject into the API container via environment variables
  # @default -- "e.g. DIRACX_CONFIG_BACKEND_URL=..."
  settings:
    # -- This corresponds to the basic dirac.cfg
    # which must be present on all the servers
    #TODO: autogenerate all of these
    DIRACX_CONFIG_BACKEND_URL: "git+file:///cs_store/initialRepo"
    DIRACX_SERVICE_AUTH_TOKEN_KEYSTORE: "file:///keystore/jwks.json"
    DIRACX_SERVICE_AUTH_ALLOWED_REDIRECTS: '["http://anything:8000/docs/oauth2-redirect"]'

  # If mysql is enabled, you are not allowed
  # to set the username passwords
  sqlDbs:
    default:
    #     rootUser: admin
    #     rootPassword: hunter123
    #     user: dirac
    #     password: password123
    #     host: sqlHost:123
    # -- Which DiracX MySQL DBs are used?
    dbs:
  #    AuthDB:
  #      internalName: DiracXAuthDB
  #    JobDB:
  #    JobLoggingDB:
  #    SandboxMetadataDB:
  #    TaskQueueDB:
  #    PilotAgentsDB
  #    ProxyDB:
  #      user: proxyUser
  #      password: hush
  #      host: proxyHost:345

  # If opensearch is enabled, you are not allowed
  # to set the username passwords
  osDbs:
    default:

    # -- Which DiracX OpenSearch DBs are used?
    dbs:
      # JobParametersDB:
      # PilotLogsDB:

  # -- List of install specifications to pass to pip before launching each container
  pythonModulesToInstall: []
  # Service
  service:
    port: 8000

ingress:
  enabled: true
  className: "nginx"
  tlsSecretName: myingress-cert
  annotations: {}

diracxWeb:
  service:
    port: 8080
  # -- install specification to pass to npm before launching container
  repoURL: ""
  branch: ""
  # -- resources to use for building the webapp if diracxWeb.repoURL is set
  installDiracxWeb:
    resources:
      requests:
        memory: 512Mi
      limits:
        memory: 1Gi

##########################

opensearch:
  enabled: true
  opensearchJavaOpts: "-Xms256m -Xmx256m"
  # replicas: 1
  singleNode: true
  config:
    # @ignored
    opensearch.yml: |
      cluster.name: opensearch-cluster

      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0

      # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
      # Implicitly done if ".singleNode" is set to "true".
      # discovery.type: single-node

      # Start OpenSearch Security Demo Configuration
      # WARNING: revise all the lines below before you go into production
      plugins:
        security:
          ssl:
            transport:
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
              enforce_hostname_verification: false
            http:
              enabled: true
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
          allow_unsafe_democertificates: true
          allow_default_init_securityindex: true
          authcz:
            admin_dn:
              - CN=kirk,OU=client,O=client,L=test,C=de
          audit.type: internal_opensearch
          enable_snapshot_restore_privilege: true
          check_snapshot_restore_write_privileges: true
          restapi:
            roles_enabled: ["all_access", "security_rest_api_access"]
          system_indices:
            enabled: true
            indices:
              [
                ".opendistro-alerting-config",
                ".opendistro-alerting-alert*",
                ".opendistro-anomaly-results*",
                ".opendistro-anomaly-detector*",
                ".opendistro-anomaly-checkpoints",
                ".opendistro-anomaly-detection-state",
                ".opendistro-reports-*",
                ".opendistro-notifications-*",
                ".opendistro-notebooks",
                ".opendistro-asynchronous-search-response*",
              ]
      ######## End OpenSearch Security Demo Configuration ########
      cluster:
        routing:
          allocation:
            disk:
              threshold_enabled: "true"
              watermark:
                flood_stage: 200mb
                low: 500mb
                high: 300mb

  resources:
    requests:
      cpu: "100m"
      memory: "100Mi"

##########################

minio:
  enabled: true
  service:
    type: NodePort
  consoleService:
    type: NodePort
  ingress:
    enabled: false
  consoleIngress:
    enabled: false
  resources:
    requests:
      memory: 512Mi
  replicas: 1
  persistence:
    enabled: false
  mode: standalone
  #TODO switch to secret if we can ?
  rootUser: rootuser
  rootPassword: rootpass123
  environment:
    MINIO_BROWSER_REDIRECT_URL: http://anything:32001/

##########################

dex:
  enabled: true
  https.enabled: false
  image:
    tag: v2.37.0

  service:
    type: NodePort
    ports:
      http:
        port: 8000
        nodePort: 32002

  ingress:
    enabled: false

  config:
    issuer: http://anything:32002

    storage:
      type: sqlite3
      config:
        file: /tmp/dex.db

    web:
      http: 8000

    expiry:
      deviceRequests: 5m
      signingKeys: 6h
      idTokens: 24h
      authRequests: 24h

    logger:
      level: "debug"
      format: text

    oauth2:
      responseTypes: [code]
      skipApprovalScreen: false
      alwaysShowLoginScreen: false

    enablePasswordDB: true

    staticClients: []
    staticPasswords: []

##########################

indigoiam:
  enabled: false
  config:
    issuer: http://anything:32003
    initial_client:
      id: null
      secret: null
  image:
    repository: indigoiam/iam-login-service
    tag: v1.8.3.rc.20231211
  service:
    type: NodePort
    port: 8080
    nodePort: 32003

##########################

mysql:
  enabled: true
  auth:
    existingSecret: mysql-secret
    username: sqldiracx
    createDatabase: false
  initdbScriptsConfigMap: mysql-init-diracx-dbs

  # if mysql pod is failing and restarting due to mysql update
  # it can be that the prob failure treshold is too low
  # increasing this number can help:
  #
  # startupProbe:
  #   enabled: true
  #   initialDelaySeconds: 15
  #   periodSeconds: 10
  #   timeoutSeconds: 1
  #   failureThreshold: 30
  #   successThreshold: 1

##########################

rabbitmq:
  enabled: true
  # Security context must be set to run on some k8s clusters (e.g. openshift)
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  auth:
    existingPasswordSecret: rabbitmq-secret
    existingErlangSecret: rabbitmq-secret

cert-manager:
  enabled: true
  installCRDs: true

cert-manager-issuer:
  enabled: true

##########################

#########################################################
#
# OPENTELEMETRY START
#
# This is highly experimental, and will change very often
# For convenience, the values that are filled here are valid
# only for run_demo.sh
#
#
#########################################################

# https://www.otelbin.io/
opentelemetry-collector:
  enabled: false
  mode: deployment
  #internalTrafficPolicy:Local

  # # For opensearch https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/opensearchexporter
  # extensions:
  #   basicauth/client:
  #   client_auth:
  #     username: admin
  #     password: admin

  # These presets control what information from the cluster itselfs are taken
  presets:
    # enables the k8sattributesprocessor and adds it to the traces, metrics, and logs pipelines
    kubernetesAttributes:
      enabled: false
    # enables the kubeletstatsreceiver and adds it to the metrics pipelines
    kubeletMetrics:
      enabled: false
    # Enables the filelog receiver and adds it to the logs pipelines
    logsCollection:
      enabled: false

  # https://github.com/open-telemetry/opentelemetry-operator/issues/931
  # Expose the port such that prometheus can scrape
  ports:
    promexp:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      hostPort: 8889
      protocol: TCP

  #Example taken from https://opentelemetry.io/docs/collector/configuration/

  config:
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      jaeger: null
      prometheus: null

    exporters:
      otlp/jaeger:
        endpoint: diracx-demo-jaeger-collector:4317
        tls:
          insecure: true
      logging:
        loglevel: debug

      prometheus:
        endpoint: ":8889"
        #namespace: test-space
        # const_labels:
        #   cst_label1: value1
        send_timestamps: true
        metric_expiration: 180m
        resource_to_telemetry_conversion:
          enabled: true

      # prometheusremotewrite:
      #   endpoint: "http://diracx-demo-prometheus-pushgateway:9091/api/v1/push"
      #   external_labels:
      #     label_name1: label_value1
      #     label_name2: label_value2

      # Unfortunately, OpenTelemetry cannot yet send data to OpenSearch directly
      #
      # https://opentelemetry.io/docs/collector/custom-collector/
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/7905
      # https://opensearch.org/blog/distributed-tracing-pipeline-with-opentelemetry/
      #
      # So we use elastic search
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/elasticsearchexporter
      elasticsearch/log:
        endpoints: [https://elastic:elastic@elasticsearch-master:9200]
        logs_index: diracx_otel_logs_index
        sending_queue:
          enabled: true
          num_consumers: 20
          queue_size: 1000
        tls:
          insecure_skip_verify: true
        # insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp/jaeger, logging]
        metrics:
          receivers: [otlp]
          exporters: [prometheus, logging]
        logs:
          receivers: [otlp]
          exporters: [elasticsearch/log, logging]

##########################

# Elasticsearch is ONLY for the opentelemetry usage as opensearch isn't ready
elasticsearch:
  enabled: false
  esJavaOpts: "-Xms128m -Xmx128m"
  replicas: 1
  clusterHealthCheckParams: "local=true"
  discovery.seed_hosts: ["elasticsearch-master-headless"]
  # Allocate smaller chunks of memory per pod.
  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "1000m"
      memory: "512M"

  # Request smaller persistent volumes.
  volumeClaimTemplate:
    accessModes: ["ReadWriteOnce"]
    storageClassName: "standard"
    resources:
      requests:
        storage: 100M

  secret:
    password: "elastic"

##############################

jaeger:
  enabled: false

  provisionDataStore:
    cassandra: false
  allInOne:
    enabled: true
  storage:
    type: none
  agent:
    enabled: false
  collector:
    enabled: false
  query:
    enabled: false

##########################
grafana:
  enabled: false
  # To get the password for admin
  # kubectl get secrets diracx-demo-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

  service:
    type: NodePort
    port: 32004
    nodePort: 32004

  # TODO: we currently cannot load the dashboards using the "dashboards" section, which would be much simpler,
  # because of https://github.com/grafana/helm-charts/issues/27
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      folder: /var/lib/grafana/dashboards/default

  # data source jaeger with url http://diracx-demo-jaeger-query:16686
  # https://grafana.com/docs/grafana/latest/datasources/jaeger/#provision-the-data-source
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Jaeger
          type: jaeger
          url: http://diracx-demo-jaeger-query:16686
        - name: Prometheus
          type: prometheus
          url: http://diracx-demo-prometheus-server:80
        - name: Elasticsearch
          type: elasticsearch
          url: https://elasticsearch-master:9200
          basicAuth: true
          basicAuthUser: elastic
          database: diracx_otel_logs_index
          isDefault: false
          jsonData:
            esVersion: 8.5.1
            logMessageField: full_message
            timeField: "@timestamp"
            timeout: 300
            tlsSkipVerify: true
            maxConcurrentShardRequests: 10
          secureJsonData:
            basicAuthPassword: elastic

##########################

prometheus:
  enabled: false
  # Use emptydir to store the data
  server:
    persistentVolume:
      enabled: false

  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false

  serverFiles:
    prometheus.yml:
      scrape_configs:
        - job_name: otel
          scrape_interval: 10s
          static_configs:
            - targets:
                - diracx-demo-opentelemetry-collector:8889

#######################################################
#
# OPENTELEMETRY END
#
#######################################################

# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
# limits:
#   cpu: 100m
#   memory: 128Mi
# requests:
#   cpu: 100m
#   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
